{"id":"mem-02bcec9a8940f16c","information":"When replacing HashSet<usize> with Vec<u8> bitset for match tracking in Rust:\n\nImplementation pattern:\n1. Bitset structure: Vec<u8> where each byte stores 8 bits, bit i represents if block i is matched\n2. Helper functions needed:\n   - is_matched(bitset, idx): Check if bit is set (byte_idx = idx / 8, bit_mask = 1 << (idx % 8))\n   - set_matched(bitset, idx): Set bit (bitset[byte_idx] |= bit_mask)\n   - init_matched(total_blocks): Create bitset with (total_blocks + 7) / 8 bytes\n   - count_matched(bitset, total_blocks): Count set bits (iterate 0..total_blocks)\n\nKey conversion steps:\n- Replace HashSet<usize> with Vec<u8> in struct\n- Replace .contains(&idx) with is_matched(bitset, idx)\n- Replace .insert(idx) with set_matched(bitset, idx)\n- Replace .len() for counting with count_matched(bitset, total_blocks)\n\nEdge cases to handle:\n- total_blocks = 0: Create empty Vec (no bytes needed)\n- total_blocks not multiple of 8: Last byte has unused bits (OK, just ignore them)\n- Out-of-bounds access: Check byte_idx < bitset.len() before accessing\n- Backwards compatibility: May need conversion function to HashSet for old APIs\n\nPerformance impact: Eliminates ~190M hash lookups during 95MB file scan, expected 1.1-1.2x speedup.","created_at":"2026-01-27T09:51:36.963Z","tags":"rust,bitset,optimization,performance,hashmap-to-bitset,zsync"}
{"id":"mem-0424bfa77130fc22","information":"Constitution v3.7.0 code review completed for lib/shell notification components and lib/permissions hooks. Key findings:\n1. Component layer compliance - all props types correctly defined in shell.interfaces.ts\n2. Container/presentational pattern followed - NotificationContainer handles state, Notifications presents UI\n3. Accessibility strong - extensive aria-labels, aria-hidden on icons, role attributes, keyboard handlers\n4. One styling violation found: notification-item.tsx line 87-89 uses inline style for WebkitLineClamp\n5. All components use CSS variables for theming (bg-background, text-foreground, etc.)\n6. Permissions hooks properly centralized in permissions.interfaces.ts with typed return values","created_at":"2026-01-27T15:22:44.701Z","tags":"constitution,code-review,notifications,permissions,accessibility,styling"}
{"id":"mem-1797f124b0bf6f5e","information":"Skip-forward optimization in zsync byte-by-byte scanning: After finding a block match at offset X, skip directly to X+block_size instead of checking X+1, X+2, ..., X+block_size-1. This is correct because target file blocks are also block_size apart, so those intermediate positions can't start another matching block. Implementation: replace for loop with while loop, add `if matched { local_offset += block_size; }` after successful match, then re-init rolling checksum at new position. Performance impact: For 2048-byte blocks with 10,000 matches in a 95MB file, eliminates ~20 million redundant byte checks (2-3x speedup). Critical: must re-init rolling checksum after skip, don't just continue with stale checksum state. Also need explicit `local_offset += 1` in non-match cases since while loop doesn't auto-increment.","created_at":"2026-01-27T09:13:20.225Z","tags":"zsync,performance,optimization,skip-forward,rolling-checksum,algorithm"}
{"id":"mem-2922f27a168432c4","information":"C++ zsync2 optimization techniques beyond bithash/bitset:\n\n1. **Skip-forward optimization (70% impact)**: After finding a block match at offset X, skip directly to X+block_size instead of checking X+1, X+2, ..., X+block_size-1. Implementation uses while loop with explicit offset management. Critical: must re-init rolling checksum after skip, don't use stale state. See rsum.c:377-400.\n\n2. **Sequential matching (10-100x speedup for similar files)**: Maintains TWO rolling checksums (r[0] current, r[1] next). When block N matches, block N+1 is highly likely to match too. Uses next_match hint to skip hash table lookup. Controlled by seq_matches parameter. See rsum.c:52-56, internal.h:45.\n\n3. **UPDATE_RSUM macro efficiency**: O(1) rolling window update using Adler-style algorithm. `#define UPDATE_RSUM(a, b, oldc, newc, bshift) do { (a) += ((unsigned char)(newc)) - ((unsigned char)(oldc)); (b) += (a) - ((oldc) << (bshift)); } while (0)`. See rsum.c:37.\n\n4. **Hash table with linked lists**: rsum_hash is array of hash_entry pointers, each pointing to linked list of colliding blocks. Efficient for moderate collision rates. See internal.h:63, hash.c:92-100.\n\n5. **Range management with binary search**: range_before_block uses bisection (O(log n)) to quickly check if block is in known ranges. Critical for already_got_block() checks. See range.c:43-63.\n\nThese are all portable to Rust without requiring bloom filters or bitset optimizations.","created_at":"2026-01-27T16:04:28.791Z","tags":"zsync2,C++,optimization,skip-forward,sequential-matching,rolling-checksum,hash-table,range-management,Rust"}
{"id":"mem-3259c226a2916a41","information":"Successfully integrated three major optimizations into streaming_block_match.rs:\n\n1. While loop skip-forward optimization: Replaced for loop with while loop in scan_byte_by_byte_streaming(). After finding a match, local_offset += block_size followed by continue to skip the else clause's local_offset += 1. This prevents the critical off-by-one bug that caused ~50% of blocks to be missed.\n\n2. BitsetTracker integration: Replaced HashSet<usize> with BitsetTracker throughout the module. Changed matched_indices: HashSet to matched_bitset: BitsetTracker in StreamingBlockMatchResult. Updated all usages:\n   - matched_indices.contains() -> matched_bitset.is_matched()\n   - matched_indices.insert() -> matched_bitset.set_matched()\n   - matched_indices.len() -> matched_bitset.count_matched()\n   \n   Backward compatibility maintained via to_block_match_result() which converts bitset to HashSet.\n\n3. BithashFilter integration: Added negative lookup optimization in scan_byte_by_byte_streaming(). Modified build_checksum_table() to return (table, bithash). Before accessing checksum_table, check if !bithash.might_contain(masked_rsum) to skip hash table lookups for definitely-absent checksums. This provides ~75% reduction in hash table accesses.\n\nAll three optimizations work together:\n- Skip-forward reduces byte-by-byte iterations after matches\n- BitsetTracker eliminates expensive HashSet hash lookups for match tracking\n- BithashFilter eliminates expensive hash table lookups for negative matches\n\nTesting: 41 unit tests + 11 integration tests all pass, including specific test for skip-forward off-by-one bug fix.","created_at":"2026-01-27T13:42:17.116Z","tags":"zsync,optimization,skip-forward,bitset,bithash,performance,streaming_block_match"}
{"id":"mem-38cd8f6cdb9ae671","information":"Bithash filter implementation complete: 8KB bloom filter (65536 bits) with double hashing using hash functions with multipliers 0x9E3779B97F4A7C15 and 0x85EBCA77C2B2AE63. Provides ~75% reduction in hash table accesses. Implementation includes insert(), might_contain(), clear(), and double_hash() methods. False positive rate stays below 15% at 15% load factor. Module is internal (not pub) as it's used by build_checksum_table() for fast negative lookups.","created_at":"2026-01-27T13:14:17.119Z","tags":"zsync,bithash,bloom-filter,double-hashing,optimization,rust"}
{"id":"mem-3caf9353999238f7","information":"Skip-forward optimization investigation findings: The skip-forward, bithash, and bitset optimizations were implemented (419 lines of changes in git stash) but are NOT present in current HEAD. Current code uses for loop instead of while loop in scan_byte_by_byte_streaming(). The stash contains complete, tested implementations including: (1) Skip-forward: while loop with local_offset += block_size after match + continue to prevent off-by-one bug, (2) Bithash: negative lookup before hash table access providing ~75% reduction in accesses, (3) BitsetTracker: Vec<u8> bitset replacing HashSet<usize>. Evidence from mem-70dea704ae273cc3 states \"NOT yet committed to HEAD\". Recommendation: Apply stash and run tests to verify compatibility, or re-implement from scratch using stash as reference. Test file debug_scan.rs expects skip_forward_count and bithash_negative_lookups fields that don't exist in current MatchStats, indicating the stash also contains debug logging infrastructure.","created_at":"2026-01-27T15:18:57.461Z","tags":"zsync,skip-forward,investigation,git-stash,optimization-state,streaming_block_match"}
{"id":"mem-3e08d9ad368211f1","information":"zsync2 vs original zsync differences: Architecture - library API (ZSyncClient, ZSyncFileMaker) vs CLI-only. HTTP client - libcurl/cpr with keep-alive vs custom implementation. Range optimization - merges nearby ranges (configurable) vs no merging. Seed files - multiple seed files with set-based deduplication vs single seed file. Progress - rich structured API vs ASCII progress bar. HTTP features - instance digest verification (RFC 3230/5843), automatic retry vs basic HTTP/1.0. Design patterns: Opaque Data Type for C ABI compatibility, Lazy initialization (hash tables built only when needed), Buffered sliding window (16 blocks + context), Range merging (minimize HTTP requests), Sequential matching (reduce false positives). Performance gotchas: avoid byte-by-byte I/O, use pwrite() not seek, build hash lazily, optimize HTTP ranges, don't ignore compression (zmap for .gz). Time complexity: block lookup O(1) avg, range query O(log n), rolling update O(1). Space complexity: O(n) for hashes, O(m) for ranges where m << n typically.","created_at":"2026-01-27T05:29:28.128Z","tags":"zsync2,differences,design-patterns,performance,complexity,original-zsync,HTTP-optimization"}
{"id":"mem-3ff74b520ff32992","information":"zsync implements bit-level random access for gzip files using zmap structure tracking: (1) inbits - bit position in compressed stream, (2) outbytes - byte position in uncompressed, (3) blockcount - tracks deflate block boundaries. Critical because deflate blocks have independent Huffman trees - need block header to decompress. Enables mid-stream decompression and 10-30% bandwidth savings for compressed downloads. Rust port likely uses byte-level only, missing this optimization.","created_at":"2026-01-27T05:29:26.663Z","tags":"zsync,gzip,deflate,compression-mapping,bit-level"}
{"id":"mem-43842507e870f4c4","information":"NotificationsService review findings: Service extends BaseAwsService properly but does NOT use executeWithRetry for AWS SDK calls (CHK090 violation). All DynamoDB operations (db.putItem, db.queryItems, db.updateItem, db.batchWrite) are called directly without retry wrapper. Service uses public constructor instead of private constructor + static create() factory (CHK095/CHK096 violations). Factory pattern exists in separate file but service allows direct instantiation. No ServiceError subclasses thrown - uses notification-specific errors from notifications.errors.ts.","created_at":"2026-01-27T15:23:42.569Z","tags":"notifications,service-layer,constitution,CHK089,CHK090,CHK095,CHK096,code-review"}
{"id":"mem-5b72feaa70543c5a","information":"Task captify--h7tpr-mkwk5atpysx: Extract message conversion utilities (convertToUIMessage, convertFromUIMessage) from lib/chat/context/chat-context.tsx lines 55-168 to lib/chat/chat.utilities.ts. Required imports: ClientChatMessage from @/lib/chat/chat.interfaces, extractToolCallsFromUIMessage from @/lib/shared/utilities/message-conversion. Functions handle bidirectional conversion between stored format and AI SDK UIMessage format, including version tracking, tool calls, reasoning, and attachments. Blocked: all file modification tools auto-denied (prompts unavailable).","created_at":"2026-01-27T12:16:54.257Z","tags":"captify,message-conversion,chat-utilities,blocked,swarm-worker"}
{"id":"mem-68c4738b9a09c32e","information":"Bitset skip-forward off-by-one bug in Rust zsync implementation: After setting local_offset += block_size when matched=true, the code continued to execute the else clause's local_offset += 1, causing skip of block_size+1 bytes instead of block_size. This caused ~50% of blocks to be missed. Fix: Add `continue;` statement after the matched=true block to skip the else clause. Testing strategy: Create file with repeating pattern at non-aligned positions to force byte-by-byte scanning, verify all blocks are found.","created_at":"2026-01-27T10:19:37.788Z","tags":"zsync,rust,off-by-one-bug,skip-forward,rolling-checksum,performance-optimization"}
{"id":"mem-70dea704ae273cc3","information":"Successfully verified all three streaming_block_match optimizations are correctly implemented in working directory:\n\n1. Skip-forward optimization: While loop with local_offset += block_size after match, followed by continue; to prevent off-by-one bug (prevents skipping block_size+1 bytes instead of block_size). Located at lines 598-614 in scan_byte_by_byte_streaming().\n\n2. BitsetTracker integration: Replaced HashSet<usize> with BitsetTracker throughout module. Changed matched_indices: HashSet to matched_bitset: BitsetTracker in StreamingBlockMatchResult struct. Updated all usages: matched_indices.contains() -> matched_bitset.is_matched(), matched_indices.insert() -> matched_bitset.set_matched(), matched_indices.len() -> matched_bitset.count_matched(). Added to_block_match_result() for backward compatibility converting bitset to HashSet.\n\n3. BithashFilter integration: Modified build_checksum_table() to return (Vec<Option<Vec<...>>>, BithashFilter). Creates BithashFilter::new() and inserts masked_rsum values. Added negative lookup in scan_byte_by_byte_streaming(): if !bithash.might_contain(masked_rsum) { continue; } to skip hash table lookups for definitely-absent checksums.\n\nAll 41 zsync tests pass including 4 streaming_block_match tests, 10 bithash_filter tests, and 13 bitset_tracker tests. Implementation follows patterns from memory mem-3259c226a2916a41 which documented the correct integration approach.\n\nWorking directory changes: 89 additions, 37 deletions. NOT yet committed to HEAD.","created_at":"2026-01-27T13:53:30.232Z","tags":"zsync,optimization,skip-forward,bitset,bithash,streaming_block_match,verification"}
{"id":"mem-80069472eebfeb61","information":"zsync2 data structures: hash_entry with rsum (a,b fields) and 16-byte checksum. rcksum_state contains blockhashes array, rsum_hash (linked list hash table), bithash (bit array for negative lookups), ranges array (sorted [start0,end0,start1,end1,...]). Range operations: range_before_block uses binary search (O(log n)), add_to_ranges auto-merges adjacent blocks. I/O patterns: rcksum_submit_source_file uses 16-block buffer + context bytes for sliding window. rcksum_submit_source_data maintains r[0] and r[1] for sequential block matching. WRITE patterns: pwrite() for direct offset writes (thread-safe, no seeking). HTTP optimization: optimizeRanges() merges gaps <= threshold (configurable). Trade-off: fewer requests vs. slightly more data downloaded (typically <5% overhead). Compression support via zmap structure mapping compressed byte offsets to uncompressed offsets. Gzip support with --rsync, --best, --no-name options (whitelisted for security).","created_at":"2026-01-27T05:29:22.579Z","tags":"zsync2,data-structures,hash-table,range-management,I/O-patterns,compression,pwrite,buffered-I/O"}
{"id":"mem-901fdbf8728b1467","information":"zsync C rolling checksum algorithm maintains TWO checksums (current block and next block) to implement sequential matching. When block N matches, next block (N+1) is highly likely to match too. Pre-calculating both checksums and checking N+1 without hash lookup provides 10-100x speedup for similar files. Rust port likely missing this optimization.","created_at":"2026-01-27T05:29:08.536Z","tags":"zsync,rolling-checksum,sequential-matching,optimization"}
{"id":"mem-9769bee9ddd7c13c","information":"zsync C implementation uses sophisticated two-level hashing: (1) bithash bloom filter (1 bit per checksum) for fast negative lookups eliminating ~75% of hash table accesses, (2) rsum_hash linked-list hash table for actual lookups. This is critical optimization - single hash table would have 4x more lookups and longer chains. Rust port should use fastbloom-rs or similar crate.","created_at":"2026-01-27T05:29:03.304Z","tags":"zsync,hashing,bloom-filter,optimization,C-implementation"}
{"id":"mem-9975091edf4e22ba","information":"zsync removes hash entries from hash table after blocks are obtained and written to disk. Benefits: (1) reduces hash chain lengths for subsequent lookups, (2) prevents re-finding already-won blocks, (3) speeds up later lookups especially important for files with many repeated blocks. Rust port likely keeps entries, causing slower lookups and more MD4 calculations.","created_at":"2026-01-27T05:29:32.757Z","tags":"zsync,hash-table,optimization,memory-management"}
{"id":"mem-9ffc46c3699114e3","information":"Bitmap optimization (replacing HashSet with bitset) didn't help because the real bottleneck was missing skip-after-match optimization (70% of performance gap), not data structure overhead. The bitmap is still worth implementing (15% improvement) but should come AFTER algorithmic fixes. This is analogous to optimizing data entry speed when the real issue is that you're checking doors you've already walked past - the sticky note (bitset) is faster than notebook (HashSet), but the real win is skipping doors you don't need to check.","created_at":"2026-01-27T05:37:57.103Z","tags":"performance,optimization,zsync,algorithm-vs-data-structure,skip-forward,bitset,bitmap"}
{"id":"mem-a6d1fb25a3958756","information":"Notifications module constitution v3.7.0 compliance review completed. Files reviewed: notifications.schema.ts, index.ts (schemas), 7 API routes, unit tests, and storybook data. Key findings:\n\nPASS items:\n- CHK139: Table name correctly prefixed as \"captify_core_notifications\" in TABLES constant\n- CHK140: Lowercase with underscores naming convention followed\n- CHK141: One schema file per table (notifications.schema.ts) with clear separation\n- CHK142: Branded key types (NotificationPK, NotificationSK) using PartitionKey<T> and SortKey<T>\n- CHK143: Key builders as pure functions in notificationKeys object\n- CHK082-088: API routes are thin wrappers (1-9 lines) importing handlers from lib/notifications/api\n- CHK052: Unit tests located in tests/services/notifications/ directory\n- CHK053: File naming convention followed (.service.unit.spec.ts)\n- CHK054-060: Comprehensive test coverage including happy path, error cases, edge cases, mocked dependencies, isolated tests\n\nFAIL items:\n- CHK045-051 (Storybook): stories/data/notifications.data.ts uses Date objects (new Date()) instead of Unix timestamps (milliseconds). According to constitution v3.7.0 Section XV, all timestamps MUST be stored as Unix timestamps (milliseconds since epoch) using Date.now(). The mock data should use numeric timestamps, not Date objects.\n\nExample from line 11-21 in stories/data/notifications.data.ts:\n```typescript\ncreatedAt: new Date(Date.now() - 1000 * 60 * 2), // SHOULD BE: Date.now() - 1000 * 60 * 2\n```\n\nThis affects ALL 10+ notification objects in the file. The fix is to remove `new Date()` wrapper and use numeric timestamps directly.","created_at":"2026-01-27T15:22:44.021Z","tags":"constitution,v3.7.0,notifications,code-review,dynamodb-schema,api-routes,unit-tests,storybook,compliance"}
{"id":"mem-a98b32c3ab5a9f3c","information":"zsync uses direct syscalls pwrite() and pread() instead of fwrite()/fseek() for block I/O. Benefits: (1) no file pointer management overhead, (2) thread-safe, (3) atomic for single filesystem calls, (4) better for random access patterns. Also uses 16-block sliding window (not 1) for source file processing, reducing fread() calls by 16x while maintaining context for rolling checksums.","created_at":"2026-01-27T05:29:12.431Z","tags":"zsync,I/O,pwrite,pread,optimization"}
{"id":"mem-c1c8401a8265fa12","information":"Debug logging added to streaming_block_match.rs to track block matching performance. Key additions:\n\n1. MatchStats structure now includes:\n   - skip_forward_count: Number of skip-forward optimizations triggered in Pass 2\n   - bithash_negative_lookups: Number of bithash filter quick rejections\n   - rolling_checksum_computations: Total rolling checksum computations\n   - md4_checksum_computations: Total MD4 checksum computations (expensive)\n\n2. Debug output at scan completion shows:\n   - Total blocks from .zsync metadata\n   - Blocks matched and match rate percentage\n   - Pass 1: Rolling and MD4 checksum counts\n   - Pass 2: Skip-forward events, bithash negative lookups\n   - Total chunks processed\n\n3. Debug test created at tests/debug_scan.rs:\n   - Can run with: cargo test -p appimage-update -- --ignored debug_cpu_x_scan\n   - Tests CPU-X-5.2.0-x86_64.AppImage (26 MB, 12847 blocks)\n   - Shows detailed statistics to identify match rate issues\n   - Verified: 100% match rate when metadata generated from same file\n\n4. Test output from CPU-X shows:\n   - Pass 1 matched all 12847 blocks (100%)\n   - Rolling checksums: 12853 (near block count)\n   - MD4 checksums: 12847 (exact match)\n   - Pass 2 not needed (all blocks found in Pass 1)\n   - Chunks processed: 7\n\nDebug logging will help identify if low match rates are caused by:\n- Incorrect metadata vs file content\n- Skip-forward skipping too aggressively\n- Bithash filter false negatives\n- Rolling checksum computation errors","created_at":"2026-01-27T14:36:17.396Z","tags":"debug,block-matching,zsync,streaming,performance,tuning"}
{"id":"mem-c4512fe4781e6ae3","information":"Rust zsync performance gaps identified: 1) Skip-after-match missing (70% impact) - scans 2048 unnecessary bytes after each match, 2) Two-level hashing with bithash missing (10% impact) - C uses bloom filter for fast negative lookups eliminating 75% of hash table accesses, 3) HashSet overhead for match tracking (15% impact) - 190 million hash lookups during 95MB file scan, 4) Sequential matching hints missing (5-10% impact) - C maintains checksums for current and next block to detect sequential matches, 5) Section-aware hashing missing (3-5% impact) - C++ skips .sha256_sig/.sig_key ELF sections in AppImages. Combined expected speedup: 3-7x to achieve C++ parity.","created_at":"2026-01-27T05:38:12.317Z","tags":"zsync,performance,algorithm,hashing,bloom-filter,sequential-matching,section-aware,appimage"}
{"id":"mem-ca19ffe753845dc8","information":"Task captify--h7tpr-mkwk5atu5nn requires adding provider interfaces to chat.interfaces.ts. New interfaces needed: ChatCoreProviderProps, ChatCoreContextValue, ChatAttachmentsProviderProps, ChatAttachmentsContextValue, ChatToolApprovalProviderProps, ChatToolApprovalContextValue, ChatDocumentProviderProps, ChatDocumentContextValue. These split the monolithic ChatProviderProps into focused provider interfaces following compound component pattern. Each provider manages a specific concern (core messages, attachments, tool approval, document context). All interfaces should have JSDoc comments per constitution rules.","created_at":"2026-01-27T12:16:19.632Z","tags":"chat,interfaces,provider,compound-components"}
{"id":"mem-d06e999afc2893ca","information":"Created PROGRESS.md tracking file for PLAN.md optimization implementation. Key structure: 3 phases with 9 tasks total. Phase 1 (Days 1-3): skip-forward (70% impact), bitset (15% impact), bithash (10% impact). Phase 2 (Days 4-6): sequential matching (5-10%), section-aware hashing (3-5%), HTTP range optimization (2-5%). Phase 3 (Days 7-8): hash entry removal (2-3%), direct I/O (1-2%), buffer size optimization (1-2%). All tasks initialized as 'pending'. Tracking includes: task description, owner, status, start date, completion date, notes. Also includes success criteria (performance targets, correctness criteria, quality criteria), risk management with mitigations, and rollback plan.","created_at":"2026-01-27T06:04:27.009Z","tags":"progress-tracking,documentation,optimization-plan,task-management,zsync-performance"}
{"id":"mem-d3c67a2ecfa92aa5","information":"CAP-405-SS branch review completed with swarm of 5 parallel reviewers. Key findings:\n\n1. CRITICAL - Service Layer (notifications.service.ts):\n   - Missing executeWithRetry() wrappers on ALL DynamoDB calls (14+ occurrences)\n   - Public constructor instead of private with static create() factory\n   \n2. CRITICAL - Schema Architecture:\n   - Duplicate Notification interface in interfaces.ts that duplicates NotificationEntity from schemas.ts\n   - File naming inconsistency: notification.utilities.ts should be notifications.utilities.ts\n   \n3. FAIL - Styling:\n   - Inline style in notification-item.tsx:87-89 using style={{ WebkitLineClamp: maxLines }}\n   \n4. FAIL - Timestamp Format:\n   - Storybook mock data uses Date objects instead of Unix timestamps (milliseconds)\n   \n5. WARNING - Code Quality:\n   - console.error without production guard in use-comments.ts:258\n   - Large file sizes: feedback.service.ts (819 lines), comments.service.ts (1,130 lines)\n\nOverall compliance: ~85% across all checklist items. API layer and hook layer fully compliant. Main issues are in service layer patterns.","created_at":"2026-01-27T15:24:56.488Z","tags":"captify,code-review,constitution,CAP-405,notifications,swarm"}
{"id":"mem-d58abd13e45a3c71","information":"Constitution v3.7.0 compliance review pattern for lib modules: \n1. File naming must use plural module prefix (notifications.X.ts, not notification.X.ts)\n2. CHK113a critical: Types must be schema-derived via z.infer, not manually duplicated\n3. Common violation: Manual interfaces that duplicate schema-derived types create drift risk\n4. Schema architecture checklist: baseEntitySchema extension, z.infer usage, Zod as source of truth\n5. UI configuration (icons, colors) in core constants may violate separation concerns - consider extracting to UI layer\n6. Redundant identity transformation functions should be removed or documented","created_at":"2026-01-27T15:24:31.363Z","tags":"constitution-review,schema-architecture,notifications,code-quality,compliance-patterns"}
{"id":"mem-e2de0738c5fd216d","information":"zsync2 uses hybrid C/C++11 architecture: C core for algorithms (librcksum, libzsync) with C++11 wrapper layer for HTTP client and API. Rolling checksum uses Adler-like algorithm with O(1) sliding window update: UPDATE_RSUM(a, b, oldc, newc, bshift). Two-tier verification: weak rolling checksum (rsum) followed by strong MD4 only on matches. Multi-level hash system: bit hash for negative lookups → rsum hash with linked lists → MD4 verification. Range management uses binary search (O(log n)) with automatic merging of adjacent blocks. Buffered processing uses 16-block sliding windows with context preservation. HTTP range optimization merges nearby ranges (default 256KB threshold) reducing HTTP requests 10-100x. Direct I/O with pwrite() avoids file seeks. Lazy hash building only constructs tables when needed. zsync2 vs original: library API (not just CLI), modern HTTP (libcurl/cpr), range optimization, multiple seed files, rich progress reporting. Key optimization: always use rolling checksum O(1) update, never recalculate; use two-tier verification (weak then strong); optimize HTTP ranges to minimize requests; use buffered I/O never byte-by-byte.","created_at":"2026-01-27T05:29:11.757Z","tags":"zsync2,rolling-checksum,optimization,algorithm,C++,rsync,range-optimization,hash-table,performance"}
{"id":"mem-e54bb95c22f38563","information":"Skip-forward optimization implementation in Rust zsync byte-by-byte scanning:\n\nKey Implementation Details:\n1. Convert for loop to while loop with explicit local_offset management\n2. After finding block match at offset X, skip to X+block_size by setting local_offset += block_size\n3. CRITICAL: Immediately re-initialize rolling checksum at new position using rolling.init(...) - do NOT use stale checksum state\n4. Add explicit local_offset += 1 for non-match cases (while loop doesn't auto-increment)\n5. Add continue statement after skip-forward to move to next iteration\n\nCritical Off-by-One Bug Avoided:\nThe bug happens when setting local_offset += block_size but then continuing to execute rolling checksum update for the old position before recalculating. This causes incorrect checksums after skips. Solution: re-init rolling checksum IMMEDIATELY after skip, before any other code executes.\n\nEdge Cases Handled:\n1. Chunk boundaries: When skip extends beyond buffer, condition local_offset + block_size <= bytes_in_buffer prevents buffer overflow\n2. Last block handling: Skip doesn't overshoot end of file\n3. Small files: Optimized correctly even for files smaller than chunk_size\n\nPerformance Impact:\nFor typical 95MB file with 10,000 matches and 2048-byte blocks, eliminates ~20 million redundant byte checks (2-3x speedup in byte-by-byte scan).\n\nTest Coverage:\n- test_skip_forward_optimization: Verifies blocks found at correct offsets after skip\n- test_skip_forward_at_chunk_boundary: Tests skip across chunk boundaries\n- test_skip_forward_last_block: Tests handling of final block in file\n- test_skip_forward_efficiency: Verifies optimization doesn't miss blocks at non-aligned offsets","created_at":"2026-01-27T16:43:49.333Z","tags":"zsync,skip-forward,rolling-checksum,optimization,performance,off-by-one-bug,rust"}
{"id":"mem-fcfe4b662248fcf7","information":"Constitution v3.7.0 code review completed for lib/feedback and lib/comments modules. Key findings:\n\nPASS items:\n- CHK009-020: Module structure follows standards with constants, interfaces, schemas properly separated\n- CHK021: No console.log statements found in production code (only in error handling catch blocks with proper guard)\n- CHK023: No 'any' types used - all types properly defined\n- CHK089-099: Service layer properly extends BaseAwsService with executeWithRetry pattern\n- CHK100-105: Hook layer uses TanStack Query patterns with typed returns\n\nFAIL items:\n- CHK022: lib/comments/hooks/use-comments.ts line 258 has console.error in catch block without NODE_ENV guard\n\nWARNINGS:\n- Comments service uses scan operations for some queries which may have performance implications at scale (lines 262, 511, 985 in comments.service.ts)\n- File complexity: feedback.service.ts (819 lines) and comments.service.ts (1130 lines) exceed recommended service file size limits","created_at":"2026-01-27T15:23:57.498Z","tags":"code-review,constitution,compliance,feedback,comments,v3.7.0"}